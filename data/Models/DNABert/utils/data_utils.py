import torch
import os
import pandas as pd
from transformers import Trainer


class HF_dataset(torch.utils.data.Dataset):
    """
    A dataset class for the pre-trained HuggingFace transformers

    Parameters
    ----------
    input_ids : torch.tensor
        The input ids of the sequences generated by the tokenizer
    attention_masks : torch.tensor
        The attention masks of the sequences generated by the tokenizer
    labels : torch.tensor
        The labels of the sequences

    Returns
    -------
    torch.utils.data.Dataset
        A dataset compatible with the HuggingFace trainer API

    """

    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return {
            "input_ids": self.input_ids[index].clone().detach(),
            "attention_mask": self.attention_masks[index].clone().detach(),
            "labels": torch.tensor(self.labels[index]).clone().detach(),
        }

    
    
class CustomTrainer(Trainer):
    def __init__(self, model, args, train_dataloader, eval_dataloader, **kwargs):
        super().__init__(model, args, **kwargs)
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader

    def get_train_dataloader(self):
        # Return your training DataLoader
        return self.train_dataloader
    
    def get_eval_dataloader(self):
        # Return your test DataLoader
        return self.eval_dataloader
    
    
class dataset(torch.utils.data.Dataset):
    """
    A dataset class for the pre-trained HuggingFace transformers
    modified of OG
    Parameters
    ----------
    input_ids : torch.tensor
        The input ids of the sequences generated by the tokenizer
    attention_masks : torch.tensor
        The attention masks of the sequences generated by the tokenizer
    labels : torch.tensor
        The labels of the sequences

    Returns
    -------
    torch.utils.data.Dataset
        A dataset compatible with the HuggingFace trainer API

    """

    def __init__(self, input_ids, attention_masks, labels, tokenizer):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        input_ids = self.input_ids[index].clone().detach()
        attention_mask = self.attention_masks[index].clone().detach()
        labels = [self.labels[idx] for idx in index]


        return {
        "input_ids": torch.tensor(input_ids),
        "attention_mask": torch.tensor(attention_mask),
        "labels": torch.tensor(labels),
    }


        # OG dataload
        # input_ids = self.input_ids[index].clone().detach()
        # attention_mask = self.attention_masks[index].clone().detach()
        # labels = self.labels[index]
        # return {
        #     "input_ids": input_ids,
        #     "attention_mask": attention_mask,
        #     "labels": labels,
        # }


def val_dataset_generator(
    tokenizer,
    kmer_size,
    val_dir,
    maxl_len=512,
):
    """
    This function generates a validation dataset by reading each of the CSV files
    from the validation directory and yields the datasets one by one

    Parameters
    ----------
    tokenizer : transformers.PreTrainedTokenizer
        The tokenizer to be used for the dataset
    KMER : int
        The length of the K-mers to be used
    val_dir : str, optional
        The directory containing the validation CSV files, by default "data/TestData"

    Yields
    -------
    torch.utils.data.Dataset
        A dataset compatible with the HuggingFace trainer API

    """

    for file in os.listdir(val_dir):
        df_test = pd.read_csv(f"{val_dir}/{file}")
        print(file, len(df_test))
        val_kmers, labels_val = [], [] #validation kmers and validation labels

        cls = (
            "CLASS" if "CLASS" in df_test.columns else "Class"
        )  # in case the column name is "CLASS" or "Class" in the CSV file

        for seq, label in zip(df_test["SEQ"], df_test[cls]):
            kmer_seq = return_kmer(seq, K=kmer_size)
            val_kmers.append(kmer_seq)
            labels_val.append(label - 1)

        val_encodings = tokenizer.batch_encode_plus(
            val_kmers,
            max_length=maxl_len,  # max length of the sequences
            pad_to_max_length=True, # pad the sequences to the max length
            truncation=True, # truncate the sequences to the max length
            return_attention_mask=True, 
            return_tensors="pt", # return torch tensors
        )
        val_dataset = HF_dataset(
            val_encodings["input_ids"], val_encodings["attention_mask"], labels_val
        )
        yield val_dataset


def val_dataset_gene(tokenizer, kmer_size, test_data, maxl_len=512):
    val_kmers, labels_val = [], []

    for seq, label in zip(test_data["SEQ"], test_data["CLASS"]):
        kmer_seq = return_kmer(seq, K=kmer_size)
        val_kmers.append(kmer_seq)
        labels_val.append(label - 1)

    val_encodings = tokenizer.batch_encode_plus(
        val_kmers,
        max_length=maxl_len,
        pad_to_max_length=True,
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt",
    )
    val_dataset = HF_dataset(
        val_encodings["input_ids"], val_encodings["attention_mask"], labels_val
    )
    return val_dataset


def return_kmer(seq, K=6):
    """
    This function outputs the K-mers of a DNA sequence

    Parameters
    ----------
    seq : str
        A single sequence to be split into K-mers
    K : int, optional
        The length of the K-mers to be used, by default 3

    Returns
    ----------
    kmer_seq : str
        A string of K-mers separated by spaces.

    Example
    ----------
    >>> return_kmer("ATCGATCG", K=3)
    'ATC TCG CGA GAT ATC TCG'
    """

    kmer_list = []
    for x in range(len(seq) - K + 1):  # move a window of size K across the sequence
        kmer_list.append(seq[x : x + K])

    kmer_seq = " ".join(kmer_list)
    return kmer_seq


def is_dna_sequence(sequence):
    """
    This function checks if a sequence is a DNA sequence

    Parameters
    ----------
    sequence : str
        A sequence to be checked

    Returns
    ----------
    bool
        True if the sequence is a DNA sequence, False otherwise

    Example
    ----------
    >>> is_dna_sequence("ATCGATCG")
    True
    """

    valid_bases = {"A", "C", "G", "T"}
    return all(base in valid_bases for base in sequence.upper())

