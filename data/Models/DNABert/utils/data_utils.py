import torch
import os
import numpy as np
import pandas as pd
from transformers import Trainer, AutoTokenizer


# class HF_dataset(torch.utils.data.Dataset):
#     """
#     A dataset class for the pre-trained HuggingFace transformers

#     Parameters
#     ----------
#     input_ids : torch.tensor
#         The input ids of the sequences generated by the tokenizer
#     attention_masks : torch.tensor
#         The attention masks of the sequences generated by the tokenizer
#     labels : torch.tensor
#         The labels of the sequences

#     Returns
#     -------
#     torch.utils.data.Dataset
#         A dataset compatible with the HuggingFace trainer API

#     """

#     def __init__(self, input_ids, attention_masks, labels):
#         self.input_ids = input_ids
#         self.attention_masks = attention_masks
#         self.labels = labels

#     def __len__(self):
#         return len(self.labels)

#     def __getitem__(self, index):
#         return {
#             "input_ids": self.input_ids[index].clone().detach(),
#             "attention_mask": self.attention_masks[index].clone().detach(),
#             "labels": torch.tensor(self.labels[index]).clone().detach(),
#         }

class HF_dataset(torch.utils.data.Dataset):
    """
    A dataset class for Hugging Face transformers.

    Parameters
    ----------
    input_ids : torch.Tensor
        Input IDs from the tokenizer.
    attention_masks : torch.Tensor
        Attention masks from the tokenizer.
    labels : torch.Tensor or list
        The labels of each sequence.
    texts : list of str, optional
        The original sequences/k-mers (for easy retrieval).

    Returns
    -------
    torch.utils.data.Dataset
        Compatible with the Hugging Face Trainer API.
    """

    def __init__(self, input_ids, attention_masks, labels, texts=None):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels
        self.texts = texts  # Storing original sequences

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        data_dict = {
            "input_ids": self.input_ids[index].clone().detach(),
            "attention_mask": self.attention_masks[index].clone().detach(),
            "labels": torch.tensor(self.labels[index]).clone().detach(),
        }
        # Include the raw text (k-mers) if available
        if self.texts is not None:
            data_dict["text"] = self.texts[index]
        return data_dict

    
class CustomTrainer(Trainer):
    def __init__(self, model, args, train_dataloader, eval_dataloader, **kwargs):
        super().__init__(model, args, **kwargs)
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader

    def get_train_dataloader(self):
        # Return your training DataLoader
        return self.train_dataloader
    
    def get_eval_dataloader(self):
        # Return your test DataLoader
        return self.eval_dataloader
    
    
class dataset(torch.utils.data.Dataset):
    """
    A dataset class for the pre-trained HuggingFace transformers
    modified of OG
    Parameters
    ----------
    input_ids : torch.tensor
        The input ids of the sequences generated by the tokenizer
    attention_masks : torch.tensor
        The attention masks of the sequences generated by the tokenizer
    labels : torch.tensor
        The labels of the sequences

    Returns
    -------
    torch.utils.data.Dataset
        A dataset compatible with the HuggingFace trainer API

    """

    def __init__(self, input_ids, attention_masks, labels, tokenizer):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        input_ids = self.input_ids[index].clone().detach()
        attention_mask = self.attention_masks[index].clone().detach()
        labels = [self.labels[idx] for idx in index]


        return {
        "input_ids": torch.tensor(input_ids),
        "attention_mask": torch.tensor(attention_mask),
        "labels": torch.tensor(labels),
    }


        # Stock dataload
        # input_ids = self.input_ids[index].clone().detach()
        # attention_mask = self.attention_masks[index].clone().detach()
        # labels = self.labels[index]
        # return {
        #     "input_ids": input_ids,
        #     "attention_mask": attention_mask,
        #     "labels": labels,
        # }


def val_dataset_generator(
    tokenizer,
    kmer_size,
    val_dir,
    maxl_len=512,
):
    """
    This function generates a validation dataset by reading each of the CSV files
    from the validation directory and yields the datasets one by one

    Parameters
    ----------
    tokenizer : transformers.PreTrainedTokenizer
        The tokenizer to be used for the dataset
    KMER : int
        The length of the K-mers to be used
    val_dir : str, optional
        The directory containing the validation CSV files, by default "data/TestData"

    Yields
    -------
    torch.utils.data.Dataset
        A dataset compatible with the HuggingFace trainer API

    """

    for file in os.listdir(val_dir):
        df_test = pd.read_csv(f"{val_dir}/{file}")
        print(file, len(df_test))
        val_kmers, labels_val = [], [] #validation kmers and validation labels

        cls = (
            "CLASS" if "CLASS" in df_test.columns else "Class"
        )  # in case the column name is "CLASS" or "Class" in the CSV file

        for seq, label in zip(df_test["SEQ"], df_test[cls]):
            kmer_seq = return_kmer(seq, K=kmer_size)
            val_kmers.append(kmer_seq)
            labels_val.append(label - 1)

        val_encodings = tokenizer.batch_encode_plus(
            val_kmers,
            max_length=maxl_len,  # max length of the sequences
            pad_to_max_length=True, # pad the sequences to the max length
            truncation=True, # truncate the sequences to the max length
            return_attention_mask=True, 
            return_tensors="pt", # return torch tensors
        )
        val_dataset = HF_dataset(
            val_encodings["input_ids"], val_encodings["attention_mask"], labels_val
        )
        yield val_dataset


def val_dataset_gene(tokenizer, kmer_size, test_data, maxl_len=512):
    val_kmers, labels_val = [], []

    for seq, label in zip(test_data["SEQ"], test_data["CLASS"]):
        kmer_seq = return_kmer(seq, K=kmer_size)
        val_kmers.append(kmer_seq)
        labels_val.append(label - 1)

    val_encodings = tokenizer.batch_encode_plus(
        val_kmers,
        max_length=maxl_len,
        pad_to_max_length=True,
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt",
    )
    val_dataset = HF_dataset(
        val_encodings["input_ids"], val_encodings["attention_mask"], labels_val
    )
    return val_dataset


def return_kmer(seq, K=6):
    """
    This function outputs the K-mers of a DNA sequence

    Parameters
    ----------
    seq : str
        A single sequence to be split into K-mers
    K : int, optional
        The length of the K-mers to be used, by default 3

    Returns
    ----------
    kmer_seq : str
        A string of K-mers separated by spaces.

    Example
    ----------
    >>> return_kmer("ATCGATCG", K=3)
    'ATC TCG CGA GAT ATC TCG'
    """

    kmer_list = []
    for x in range(len(seq) - K + 1):  # move a window of size K across the sequence
        kmer_list.append(seq[x : x + K])

    kmer_seq = " ".join(kmer_list)
    return kmer_seq


def is_dna_sequence(sequence):
    """
    This function checks if a sequence is a DNA sequence

    Parameters
    ----------
    sequence : str
        A sequence to be checked

    Returns
    ----------
    bool
        True if the sequence is a DNA sequence, False otherwise

    Example
    ----------
    >>> is_dna_sequence("ATCGATCG")
    True
    """

    valid_bases = {"A", "C", "G", "T"}
    return all(base in valid_bases for base in sequence.upper())

def encode_data(kmers, labels, tokenizer):
    """
    Tokenizes k-mer sequences and returns an HF_dataset with original texts.

    Parameters
    ----------
    kmers : list[str] or np.ndarray
        The k-mer sequences.
    labels : list[int] or np.ndarray
        The numeric labels for each sequence.
    tokenizer : AutoTokenizer
        The tokenizer instance.

    Returns
    -------
    HF_dataset
        A dataset containing tokenized fields and original text.
    """
    # Ensure kmers is a list
    if not isinstance(kmers, list):
        kmers = kmers.tolist()

    encodings = tokenizer.batch_encode_plus(
        kmers,
        max_length=512,
        truncation=True,
        padding=True,
        return_attention_mask=True,
        return_tensors="pt"
    )

    # Convert labels to a list if needed
    if hasattr(labels, "tolist"):
        labels = labels.tolist()

    # Create HF_dataset, storing the original kmers in `texts`
    dataset = HF_dataset(
        input_ids=encodings["input_ids"],
        attention_masks=encodings["attention_mask"],
        labels=labels,
        texts=kmers
    )
    return dataset



def get_adversarial_data(model, tokenizer, dataset, device="cuda" if torch.cuda.is_available() else "cpu"):
    """
    Evaluate `dataset` using `model`, return original sequences & labels for misclassified samples.

    Parameters
    ----------
    model : nn.Module
        The PyTorch model to evaluate (must output `logits`).
    tokenizer : AutoTokenizer
        The tokenizer used for decoding if needed.
    dataset : HF_dataset
        A dataset containing `input_ids`, `attention_mask`, and `labels`. Optionally stores `text`.
    device : str
        The device to run evaluation on ("cuda" or "cpu").

    Returns
    -------
    adversarial_seqs : list[str]
        Original k-mers of the misclassified samples.
    adversarial_labels : list[int]
        The ground-truth labels for those misclassified samples.
    """
    model.to(device)
    model.eval()

    adversarial_seqs = []
    adversarial_labels = []

    # Create a DataLoader for evaluation
    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)

    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=-1)

            # Identify misclassifications
            mismatch_indices = (preds != labels).nonzero(as_tuple=True)[0]
            for idx in mismatch_indices:
                # Retrieve the original text (k-mers)
                seq_text = batch["text"][idx]
                adversarial_seqs.append(seq_text)
                adversarial_labels.append(labels[idx].item())

    return adversarial_seqs, adversarial_labels

